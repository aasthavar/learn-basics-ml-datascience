{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition:\n",
    "\n",
    "- Correlation is a highly applied technique in machine learning during data analysis and data mining. \n",
    "- It can extract key problems from a given set of features, which can later cause significant damage during the fitting model.\n",
    "- Data having non-correlated features have many benefits. Such as:\n",
    "    - Learning of Algorithm will be faster\n",
    "    - Interpretability will be high\n",
    "    - Bias will be less\n",
    "\n",
    "Interpret the Values:\n",
    "\n",
    "- Positive Correlation (> 0): As one variable increases, the other variable tends to increase.\n",
    "- Negative Correlation (< 0): As one variable increases, the other variable tends to decrease.\n",
    "- No Correlation (â‰ˆ 0): No linear relationship between the variables.\n",
    "\n",
    "Identify Strong Correlations:\n",
    "\n",
    "- Typically, correlations above 0.7 or below -0.7 are considered strong.\n",
    "- Look for pairs of variables with high absolute correlation values.\n",
    "\n",
    "Derive Insights:\n",
    "\n",
    "- Detect Multicollinearity: High correlations between independent variables can indicate multicollinearity, which can affect the performance of regression models.\n",
    "- Feature Selection: Identify and remove redundant features. For example, if two features are highly correlated, you might choose to drop one.\n",
    "- Understand Relationships: Gain insights into how variables interact with each other, which can inform feature engineering and model building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load, eda\n",
    "- preprocess\n",
    "- train models\n",
    "- feature selection and train models\n",
    "- visualize and compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load and eda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download dataset: \n",
    "- option1: link - https://www.kaggle.com/datasets/debasisdotcom/parkinson-disease-detection\n",
    "- option2: run this command \n",
    "\n",
    "    ```kaggle datasets download -d debasisdotcom/parkinson-disease-detection```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # install dependencies\n",
    "# %pip install --quiet numpy pandas scikit-learn xgboost lightgbm\n",
    "# %pip install --quiet ydata-profiling ipywidgets\n",
    "# %pip install --quiet plotly \"nbformat>=4.2.0\" statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv(\"dataset/parkinson-disease.csv\")\n",
    "\n",
    "print(f\"shape: {df.shape}\")\n",
    "print(f\"count of rows with missing values: {len(df[df.isna().any(axis=1)])}\")\n",
    "print(f\"columns: {df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check count and dtypes\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check statistics of numerical cols \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats of categorical cols\n",
    "df.describe(include=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # quick eda\n",
    "# keyword = \"train\"\n",
    "# profile = ProfileReport(df, title=f\"{keyword} dataset\")\n",
    "# profile.to_notebook_iframe()\n",
    "\n",
    "# # visualize\n",
    "# for col in df.columns:\n",
    "#     fig = px.histogram(df, x=col)\n",
    "#     fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.copy()\n",
    "\n",
    "# drop columns\n",
    "df_new.drop([\"status\"], axis=1, inplace=True)\n",
    "\n",
    "# handle categorical variables\n",
    "df_new = df_new.select_dtypes(exclude=['object'])\n",
    "\n",
    "print(df_new.shape)\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the correlation matrix\n",
    "fig = px.imshow(\n",
    "    df_new.corr(), \n",
    "    text_auto=True, aspect=\"auto\", \n",
    "    color_continuous_scale=\"RdBu\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    # drop name due to high cardinality\n",
    "    df.drop(\"name\", axis=1, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply preprocessing\n",
    "preprocessing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if non-null values == count(rows)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for colname in df.select_dtypes(\"object\"):\n",
    "    df[colname], _ = df[colname].factorize()\n",
    "\n",
    "# create feature columns\n",
    "X = df.drop([\"status\"], axis=1)\n",
    "\n",
    "# one hot encode\n",
    "# X = pd.get_dummies(X)\n",
    "# create target columns\n",
    "y = df[\"status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the correlation matrix\n",
    "fig = px.imshow(\n",
    "    X.corr(), \n",
    "    text_auto=True, aspect=\"auto\", \n",
    "    color_continuous_scale=\"RdBu\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(data, labels, pipelines):\n",
    "    results = []\n",
    "    # split dataset\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(data, labels, test_size=0.2, random_state=66)\n",
    "    \n",
    "    for clf, pipeline in pipelines.items():\n",
    "        model = pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        y_hat = model.predict(X_valid)\n",
    "        accuracy = accuracy_score(y_valid, y_hat)\n",
    "        precision = precision_score(y_valid, y_hat)\n",
    "        recall = recall_score(y_valid, y_hat)\n",
    "        \n",
    "        results.append({\n",
    "            \"classifier\": clf,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def fe_apply_correlation(data, threshold):\n",
    "    df = data.copy()\n",
    "    corr_matrix = df.corr()\n",
    "    \n",
    "    corr_columns = set()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                corr_columns.add(corr_matrix.columns[i])\n",
    "    df.drop(corr_columns, axis=1, inplace=True)\n",
    "    return df            \n",
    "\n",
    "def bench_corr_coeff(thresholds, data, labels, pipelines):\n",
    "    results = []\n",
    "    for threshold in thresholds:\n",
    "        reduced_data = fe_apply_correlation(data, threshold)\n",
    "        results_temp = train_models(reduced_data, labels, pipelines)\n",
    "        results_temp = [{**item, 'coeff_threshold': threshold} for item in results_temp]\n",
    "        results += results_temp\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {\n",
    "    \"Logistic\": make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000)),\n",
    "    \"KNeighbors\": make_pipeline(StandardScaler(), KNeighborsClassifier()),\n",
    "    \"DecisionTree\": make_pipeline(StandardScaler(), DecisionTreeClassifier(random_state=66)),\n",
    "    \"XGBoost\": make_pipeline(StandardScaler(), XGBClassifier(objective=\"binary:logistic\", random_state=66)),\n",
    "    \"LightGBM\": make_pipeline(StandardScaler(), LGBMClassifier(random_state=66)),\n",
    "    \"RandomForest\": make_pipeline(StandardScaler(), RandomForestClassifier(random_state=66, n_estimators=200)),\n",
    "    \"AdaBoost\": make_pipeline(StandardScaler(), AdaBoostClassifier(random_state=66)),\n",
    "    \"GradientBoosting\": make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state=66)),\n",
    "    \"HistGradientBoosting\": make_pipeline(StandardScaler(), HistGradientBoostingClassifier(random_state=66)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# train model with data as it is\n",
    "results = train_models(data=X, labels=y, pipelines=pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature selection + train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# apply coefficient thresholds and train models\n",
    "coeff_thresholds = [0.6, 0.7, 0.8, 0.9, 0.95, 0.99]\n",
    "coeff_results = bench_corr_coeff(thresholds=coeff_thresholds, data=X, labels=y, pipelines=pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.style.highlight_max(subset=results_df.columns[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_results_df = pd.DataFrame(coeff_results)\n",
    "coeff_results_df.groupby(\"coeff_threshold\")[[\"classifier\", \"accuracy\", \"precision\"]].apply(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_results_df.style.highlight_max(subset=coeff_results_df.columns[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original features\n",
    "X.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best threshold features\n",
    "reduced_X = fe_apply_correlation(data=X, threshold=0.8)\n",
    "reduced_X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- Best accuracy with all features: KNeighborsâ€Š-â€Š0.94\n",
    "- Best accuracy after feature selection based on correlation coefficient threshold: KNeighborsâ€Š-â€Š0.97\n",
    "- Improvement of ~3% ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
